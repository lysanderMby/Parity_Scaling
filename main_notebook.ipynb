{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of test of parity scaling laws in Jupyter notebook.\n",
    "\n",
    "The code contained here is very similar to that in the rest of the repo, but is not guarenteed to be identical.\n",
    "\n",
    "This notebook is included for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "\n",
    "# pip install if necessary\n",
    "'''\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install pathlib\n",
    "!pip install logging\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Parameters\n",
    "PARAMS = {\n",
    "    'n_tasks': 1, # number of unique tasks being trained over\n",
    "    'len_taskcode': 4, # number of bits in the task code\n",
    "    'num_checks': 5, # number of bits in the message that are used to determine the output\n",
    "    'len_message': 16, # number of bits in the message\n",
    "    'num_samples': 1000, # number of samples to generate for each task\n",
    "    'input_size': 20,  # len_taskcode + len_message. Used for model initialisation\n",
    "    'output_size': 1, # output size of the model. 1 for binary classification. Do not change\n",
    "    'learning_rate': 0.005, # constant learning rate. Could introduce a scheduler?\n",
    "    'batch_size': 32, # batch size used in training. Will be the same throughout a run\n",
    "    'flop_budget': 1e10, # total number of estimated flops expended per training run\n",
    "    'task_sample_freq': 1e5,  # the rate at which performance is evaluated. Can give a big performance hit\n",
    "    'plot_freq': 2e7,  # flop_budget/5\n",
    "    'samples_per_task': 100, # number of samples to generate for each task in evaluation\n",
    "\n",
    "    'task_distribution': 'geometric',  # can be 'uniform', 'geometric', or custom list\n",
    "    'task_distribution_parameter': 0.5  # each task is half as likely as the previous\n",
    "}\n",
    "\n",
    "# Define a range of model configurations\n",
    "model_configs = [\n",
    "    {\"num_layers\": 2, \"hidden_size\": 8},\n",
    "    {\"num_layers\": 4, \"hidden_size\": 16},\n",
    "    {\"num_layers\": 6, \"hidden_size\": 32},\n",
    "    {\"num_layers\": 8, \"hidden_size\": 64},\n",
    "    {\"num_layers\": 10, \"hidden_size\": 128},\n",
    "    {\"num_layers\": 12, \"hidden_size\": 256},\n",
    "    {\"num_layers\": 14, \"hidden_size\": 512} \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate geometric distribution\n",
    "def create_geometric_distribution(n_tasks, parameter=0.5):\n",
    "    \"\"\"\n",
    "    Create a geometric probability distribution where each task is \n",
    "    parameter times as likely as the previous task.\n",
    "    \n",
    "    Args:\n",
    "        n_tasks: Number of tasks\n",
    "        parameter: Multiplier for successive probabilities (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        List of probabilities that sum to 1\n",
    "    \"\"\"\n",
    "    # Generate unnormalized probabilities\n",
    "    unnormalized = [parameter**i for i in range(n_tasks)]\n",
    "    # Normalize to sum to 1\n",
    "    total = sum(unnormalized)\n",
    "    return [p/total for p in unnormalized]\n",
    "\n",
    "# In main or where you're setting up the training\n",
    "def get_task_probabilities(params):\n",
    "    \"\"\"Get task probabilities based on parameters.\"\"\"\n",
    "    if params.get('task_distribution') == 'uniform' or 'task_distribution' not in params:\n",
    "        return None\n",
    "    elif params['task_distribution'] == 'geometric':\n",
    "        return create_geometric_distribution(\n",
    "            params['n_tasks'], \n",
    "            params.get('task_distribution_parameter', 0.5)\n",
    "        )\n",
    "    elif isinstance(params['task_distribution'], list):\n",
    "        if len(params['task_distribution']) != params['n_tasks']:\n",
    "            raise ValueError(\"Custom distribution length must match n_tasks\")\n",
    "        if not np.isclose(sum(params['task_distribution']), 1.0):\n",
    "            raise ValueError(\"Custom distribution must sum to 1\")\n",
    "        return params['task_distribution']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_distribution specification\")\n",
    "\n",
    "# When generating data in train_and_evaluate\n",
    "task_probabilities = get_task_probabilities(params)\n",
    "[data, value] = generate_dataset(tasks_dict, num_samples, task_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "    \n",
    "def generate_random_binary_string(length):\n",
    "    # Random binary string of length 'length'\n",
    "    return ''.join(random.choice(['0', '1']) for _ in range(length))\n",
    "\n",
    "def generate_dict(n_tasks, len_taskcode, num_checks, len_message):\n",
    "    # Generate the task codes and their associated check bits\n",
    "    unique_strings = set()\n",
    "    tasks_dict = {}\n",
    "    while len(unique_strings) < n_tasks:\n",
    "        binary_string = generate_random_binary_string(len_taskcode)\n",
    "        if binary_string not in unique_strings:\n",
    "            unique_strings.add(binary_string)\n",
    "            integer_list = [random.randint(0, len_message-1) for _ in range(num_checks)]\n",
    "            tasks_dict[binary_string] = integer_list\n",
    "    return tasks_dict\n",
    "\n",
    "def generate_dataset(tasks_dict, num_samples, task_probabilities=None):\n",
    "    # Initialize arrays\n",
    "    data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "    value = np.zeros(num_samples)\n",
    "    # If no probabilities provided, use uniform distribution\n",
    "    if task_probabilities is None:\n",
    "        task_probabilities = [1/len(tasks_dict)] * len(tasks_dict)\n",
    "    # Verify probability distribution\n",
    "    if len(task_probabilities) != len(tasks_dict):\n",
    "        raise ValueError(f\"Length of task_probabilities ({len(task_probabilities)}) \"\n",
    "                       f\"must match number of tasks ({len(tasks_dict)})\")\n",
    "    if not np.isclose(sum(task_probabilities), 1.0):\n",
    "        raise ValueError(f\"Task probabilities must sum to 1, got {sum(task_probabilities)}\")\n",
    "    # Convert tasks_dict keys to list for numpy.random.choice\n",
    "    tasks_list = list(tasks_dict.keys())\n",
    "    # Generate samples\n",
    "    for i in range(num_samples):\n",
    "        # Select task according to probability distribution\n",
    "        rand_task = np.random.choice(tasks_list, p=task_probabilities)\n",
    "        rand_checkbits = tasks_dict[rand_task]\n",
    "        # Generate message and compute parity\n",
    "        message = generate_random_binary_string(len_message)\n",
    "        parity_bit = sum(int(message[j]) for j in rand_checkbits) % 2\n",
    "        # Store data and value\n",
    "        data[i] = np.concatenate((np.array(list(rand_task)), np.array(list(message))))\n",
    "        value[i] = parity_bit\n",
    "    return [data, value]\n",
    "\n",
    "def generate_dataset_for_task(task_code, tasks_dict, num_samples, len_taskcode, len_message):\n",
    "    # Generate a dataset of num_samples samples for a specific task\n",
    "    # Used primarily for evaluation. Very limited performance improvement from generate_dataset\n",
    "    data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "    value = np.zeros(num_samples)\n",
    "    rand_checkbits = tasks_dict[task_code]\n",
    "    for i in range(num_samples):\n",
    "        message = generate_random_binary_string(len_message)\n",
    "        parity_bit = sum(int(message[j]) for j in rand_checkbits) % 2\n",
    "        data[i] = np.concatenate((np.array(list(task_code)), np.array(list(message))))\n",
    "        value[i] = parity_bit\n",
    "    return [data, value]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, device):\n",
    "        # Convert to numpy first for efficiency\n",
    "        data_np = dataframe.iloc[:, :-1].values\n",
    "        target_np = dataframe.iloc[:, -1].values\n",
    "        \n",
    "        # Single transfer to device. IO-aware for greater efficiency\n",
    "        self.data = torch.from_numpy(data_np).float().to(device)\n",
    "        self.target = torch.from_numpy(target_np).float().to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            if i == 0:\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if i % 2 == 0:\n",
    "                    x = F.relu(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot intermediate progress\n",
    "\n",
    "def plot_progress(loss_data, accuracy_data, task_accuracy_data, cumulative_flops, exp_dir):\n",
    "    \"\"\"\n",
    "    Plot and save training progress.\n",
    "    \n",
    "    Args:\n",
    "        loss_data: List of tuples (flops, loss)\n",
    "        accuracy_data: List of tuples (flops, accuracy)\n",
    "        task_accuracy_data: Dict of lists of tuples (flops, accuracy) for each task\n",
    "        cumulative_flops: Current total FLOPs\n",
    "        exp_dir: Path to experiment directory\n",
    "    \"\"\"\n",
    "    # Create plots directory if it doesn't exist\n",
    "    plots_dir = exp_dir / \"intermediate_plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    flops_loss, losses = zip(*loss_data)\n",
    "    ax1.plot(flops_loss, losses)\n",
    "    ax1.set_xlabel('FLOPs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss vs FLOPs')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot overall accuracy\n",
    "    flops_acc, accuracies = zip(*accuracy_data)\n",
    "    ax2.plot(flops_acc, accuracies)\n",
    "    ax2.set_xlabel('FLOPs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Overall Accuracy vs FLOPs')\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    # Plot task-specific accuracies\n",
    "    for task_idx, task_data in task_accuracy_data.items():\n",
    "        if task_data:  # Check if there's data for this task\n",
    "            flops_task, task_accuracies = zip(*task_data)\n",
    "            ax3.plot(flops_task, task_accuracies, label=f'Task {task_idx}')\n",
    "    ax3.set_xlabel('FLOPs')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Task-Specific Accuracy vs FLOPs')\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = plots_dir / f\"progress_{cumulative_flops:.2e}_flops.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Log the plot creation\n",
    "    logging.info(f\"Saved progress plot at {cumulative_flops:.2e} FLOPs to {plot_path}\")\n",
    "    \n",
    "    # Also save the data as CSV for later analysis\n",
    "    data_dir = exp_dir / \"intermediate_data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save loss and accuracy data\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'flops': flops_loss,\n",
    "        'loss': losses,\n",
    "        'accuracy': accuracies\n",
    "    })\n",
    "    df_metrics.to_csv(data_dir / f\"metrics_{cumulative_flops:.2e}_flops.csv\", index=False)\n",
    "    \n",
    "    # Save task-specific accuracy data\n",
    "    task_data_dict = {}\n",
    "    for task_idx, task_data in task_accuracy_data.items():\n",
    "        if task_data:\n",
    "            flops_task, task_accuracies = zip(*task_data)\n",
    "            task_data_dict[f'task_{task_idx}_flops'] = flops_task\n",
    "            task_data_dict[f'task_{task_idx}_accuracy'] = task_accuracies\n",
    "    \n",
    "    df_tasks = pd.DataFrame(task_data_dict)\n",
    "    df_tasks.to_csv(data_dir / f\"task_accuracies_{cumulative_flops:.2e}_flops.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main plot\n",
    "\n",
    "def main_plot(all_loss_data, all_accuracy_data, all_task_accuracy_data, all_flops):\n",
    "\n",
    "    # Create final plots\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Remove the last element of each loss list. The final step of evaluation is always cut short and not the entire batch is used?\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for i, config in enumerate(model_configs):\n",
    "        flops, losses = zip(*all_loss_data[i])\n",
    "        plt.loglog(flops, losses, label=f'{config[\"num_layers\"]}x{config[\"hidden_size\"]}')\n",
    "    plt.xlabel('Cumulative FLOPs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs FLOPs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for i, config in enumerate(model_configs):\n",
    "        flops, accuracies = zip(*all_accuracy_data[i])\n",
    "        plt.semilogx(flops, accuracies, label=f'{config[\"num_layers\"]}x{config[\"hidden_size\"]}')\n",
    "    plt.xlabel('Cumulative FLOPs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs FLOPs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for i, config in enumerate(model_configs):\n",
    "        for task in range(n_tasks):\n",
    "            flops, accuracies = zip(*all_task_accuracy_data[i][task])\n",
    "            plt.semilogx(flops, accuracies, label=f'Task {task+1} - {config[\"num_layers\"]}x{config[\"hidden_size\"]}')\n",
    "    plt.xlabel('Cumulative FLOPs')\n",
    "    plt.ylabel('Task-specific Accuracy')\n",
    "    plt.title('Task-specific Accuracies vs FLOPs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count FLOPs\n",
    "from typing import Tuple, Dict # if used, this should be moved to the import section\n",
    "\n",
    "# No longer in use\n",
    "#def count_flops_fvcore(model, input_size):\n",
    "#    input_tensor = torch.randn(2, input_size)\n",
    "#    flops = FlopCountAnalysis(model, input_tensor)\n",
    "#    return flops.total() // 2\n",
    "\n",
    "class FlopCounter:\n",
    "    def __init__(self, model: nn.Module, input_size: int, batch_size: int):\n",
    "        self.model = model\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def count_linear_flops(self, in_features: int, out_features: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count FLOPs for linear layer operations.\n",
    "        Forward: Each output element requires in_features multiplications and in_features-1 additions\n",
    "        Backward: Requires computing gradients for weights, biases, and input\n",
    "        \"\"\"\n",
    "        forward_flops = self.batch_size * out_features * (2 * in_features - 1)  # mult-add pairs\n",
    "        \n",
    "        # Backward pass FLOPs:\n",
    "        # 1. dL/dW computation: batch_size * in_features * out_features * 2\n",
    "        # 2. dL/db computation: batch_size * out_features\n",
    "        # 3. dL/dx computation: batch_size * in_features * out_features * 2\n",
    "        backward_flops = (\n",
    "            self.batch_size * in_features * out_features * 2 +  # dL/dW\n",
    "            self.batch_size * out_features +                    # dL/db\n",
    "            self.batch_size * in_features * out_features * 2    # dL/dx\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"forward\": forward_flops,\n",
    "            \"backward\": backward_flops\n",
    "        }\n",
    "    \n",
    "    def count_batch_norm_flops(self, num_features: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count FLOPs for batch normalization operations.\n",
    "        Forward: Computing mean, variance, normalized values, and scaling\n",
    "        Backward: Computing gradients for gamma, beta, and input\n",
    "        \"\"\"\n",
    "        # Forward pass operations per feature:\n",
    "        # 1. Mean calculation: batch_size additions\n",
    "        # 2. Variance calculation: batch_size multiplications and additions\n",
    "        # 3. Normalization: 4 operations per element (subtract mean, divide by std)\n",
    "        # 4. Scale and shift: 2 operations per element\n",
    "        forward_flops = self.batch_size * num_features * (7)\n",
    "        \n",
    "        # Backward pass operations:\n",
    "        # 1. Gradients for gamma and beta: batch_size additions per feature\n",
    "        # 2. Gradients for input: ~8 operations per element\n",
    "        backward_flops = self.batch_size * num_features * 10\n",
    "        \n",
    "        return {\n",
    "            \"forward\": forward_flops,\n",
    "            \"backward\": backward_flops\n",
    "        }\n",
    "    \n",
    "    def count_relu_flops(self, num_elements: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count FLOPs for ReLU activation.\n",
    "        Forward: One comparison per element\n",
    "        Backward: One multiplication per element (gradient is 0 or 1)\n",
    "        \"\"\"\n",
    "        forward_flops = num_elements  # One comparison per element\n",
    "        backward_flops = num_elements  # One multiplication per element\n",
    "        \n",
    "        return {\n",
    "            \"forward\": forward_flops,\n",
    "            \"backward\": backward_flops\n",
    "        }\n",
    "    \n",
    "    def calculate_total_flops(self) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculate total FLOPs for both forward and backward passes through the entire model.\n",
    "        Returns tuple of (forward_flops, backward_flops)\n",
    "        \"\"\"\n",
    "        total_forward_flops = 0\n",
    "        total_backward_flops = 0\n",
    "        \n",
    "        current_size = self.input_size\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                flops = self.count_linear_flops(layer.in_features, layer.out_features)\n",
    "                total_forward_flops += flops[\"forward\"]\n",
    "                total_backward_flops += flops[\"backward\"]\n",
    "                current_size = layer.out_features\n",
    "                \n",
    "            elif isinstance(layer, nn.BatchNorm1d):\n",
    "                flops = self.count_batch_norm_flops(current_size)\n",
    "                total_forward_flops += flops[\"forward\"]\n",
    "                total_backward_flops += flops[\"backward\"]\n",
    "                \n",
    "            # Count ReLU FLOPs after linear layers (except the last one)\n",
    "            if isinstance(layer, nn.Linear) and layer != self.model.layers[-1]:\n",
    "                flops = self.count_relu_flops(self.batch_size * current_size)\n",
    "                total_forward_flops += flops[\"forward\"]\n",
    "                total_backward_flops += flops[\"backward\"]\n",
    "        \n",
    "        return total_forward_flops, total_backward_flops\n",
    "\n",
    "def get_flops_per_pass(model: nn.Module, input_size: int, batch_size: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Wrapper function to get FLOPs per forward and backward pass.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        input_size: Size of input features\n",
    "        batch_size: Batch size used in training\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (forward_flops, backward_flops)\n",
    "    \"\"\"\n",
    "    counter = FlopCounter(model, input_size, batch_size)\n",
    "    return counter.calculate_total_flops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# This function has been significantly modified to incorporate the new count FLOPs function\n",
    "# There may be bugs down the line related to how data is collected and displayed. This should be more thoroughly debugged\n",
    "\n",
    "def train_and_evaluate(model, criterion, optimizer, flop_budget, tasks_dict):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize FlopCounter\n",
    "    flop_counter = FlopCounter(model, input_size=len_taskcode + len_message, batch_size=batch_size)\n",
    "    forward_flops, backward_flops = flop_counter.calculate_total_flops()\n",
    "    \n",
    "    loss_data = []\n",
    "    accuracy_data = []\n",
    "    task_accuracy_data = {i: [] for i in range(n_tasks)}\n",
    "    cumulative_flops = 0\n",
    "    epoch = 0\n",
    "    last_task_sample = 0\n",
    "    last_plot = 0\n",
    "\n",
    "    print_rate = flop_budget / 1e1\n",
    "    disp_flops = 0\n",
    "\n",
    "    while cumulative_flops < flop_budget:\n",
    "        if cumulative_flops - print_rate > disp_flops:\n",
    "            print(f'cumulative_flops: {cumulative_flops} - flop_budget: {flop_budget} - Percentage completion: {(cumulative_flops/flop_budget)*100:.2f}%')\n",
    "            disp_flops = cumulative_flops\n",
    "            \n",
    "        epoch += 1\n",
    "        [data, value] = generate_dataset(tasks_dict, num_samples)\n",
    "        df = pd.DataFrame(np.concatenate((data, value.reshape(-1, 1)), axis=1), \n",
    "                         columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "        \n",
    "        dataset = CustomDataset(df, device)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            batch_loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            predictions = (outputs >= 0.5).squeeze().long()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += batch_loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate FLOPs\n",
    "            batch_flops = forward_flops + backward_flops\n",
    "            batch_flops += forward_flops + backward_flops\n",
    "            cumulative_flops += batch_flops\n",
    "\n",
    "            if cumulative_flops >= flop_budget and i > (len(data_loader) - 1):\n",
    "                break\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataset)\n",
    "        avg_accuracy = correct / total\n",
    "\n",
    "        loss_data.append((cumulative_flops, avg_loss))\n",
    "        accuracy_data.append((cumulative_flops, avg_accuracy))\n",
    "        \n",
    "        # Task-specific evaluation\n",
    "        if cumulative_flops - last_task_sample >= task_sample_freq or cumulative_flops >= flop_budget:\n",
    "            last_task_sample = cumulative_flops\n",
    "            tasks_list = list(tasks_dict.keys())\n",
    "            \n",
    "            for task_idx, task_code in enumerate(tasks_list):\n",
    "                [data_per_task, value_per_task] = generate_dataset_for_task(task_code, tasks_dict, samples_per_task)\n",
    "                df_per_task = pd.DataFrame(np.concatenate((data_per_task, value_per_task.reshape(-1, 1)), axis=1), \n",
    "                                         columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "                dataset_per_task = CustomDataset(df_per_task, device)\n",
    "                loader_per_task = DataLoader(dataset_per_task, batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "                model.eval()\n",
    "                task_correct = 0\n",
    "                task_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in loader_per_task:\n",
    "                        outputs = model(inputs)\n",
    "                        predictions = (outputs >= 0.5).squeeze().long()\n",
    "                        task_correct += (predictions == labels).sum().item()\n",
    "                        task_total += labels.size(0)\n",
    "                        cumulative_flops += forward_flops\n",
    "                        \n",
    "                task_accuracy = task_correct / task_total\n",
    "                task_accuracy_data[task_idx].append((cumulative_flops, task_accuracy))\n",
    "        \n",
    "        if cumulative_flops - last_plot >= plot_freq:\n",
    "            last_plot = cumulative_flops\n",
    "            plot_progress(loss_data, accuracy_data, task_accuracy_data, cumulative_flops, exp_dir)\n",
    "\n",
    "    return loss_data, accuracy_data, task_accuracy_data, cumulative_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create experiment directory\n",
    "    exp_name = f\"parity_scaling_flops_{PARAMS['flop_budget']:.0e}\"\n",
    "    exp_dir = create_versioned_directory(Path(\"experiments\"), exp_name)\n",
    "    print(f\"Experiment directory: {exp_dir}\")\n",
    "\n",
    "    print(f\"FLOP budget: {PARAMS['flop_budget']}\")\n",
    "    \n",
    "    tasks_dict = generate_dict(\n",
    "        PARAMS['n_tasks'], \n",
    "        PARAMS['len_taskcode'], \n",
    "        PARAMS['num_checks'], \n",
    "        PARAMS['len_message']\n",
    "    )\n",
    "    print(f\"Generated tasks dictionary with {len(tasks_dict)} tasks\")\n",
    "    print(\"tasks_dict = \", tasks_dict.items())\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Add progress bar for model configurations\n",
    "    for config in tqdm(MODEL_CONFIGS, desc=\"Training models\", position=0, leave=True):\n",
    "        print(f\"\\nTraining model with {config['num_layers']} layers and hidden size {config['hidden_size']}\")\n",
    "        model = NeuralNetwork(\n",
    "            PARAMS['input_size'], \n",
    "            PARAMS['output_size'], \n",
    "            config[\"num_layers\"], \n",
    "            config[\"hidden_size\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        results = train_and_evaluate(\n",
    "            model=model,\n",
    "            params=PARAMS,\n",
    "            tasks_dict=tasks_dict,\n",
    "            exp_dir=exp_dir,\n",
    "            model_config=config\n",
    "        )\n",
    "        all_results.append(results)\n",
    "    \n",
    "    # Create final plots\n",
    "    main_plot(all_results, exp_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional post-processing function\n",
    "\n",
    "def create_seaborn_plots(exp_dir: Path, epoch_window: int = 10, confidence_interval: float = 0.95):\n",
    "    \"\"\"Create seaborn plots from saved data across all model configurations.\"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Directory setup and validation\n",
    "    if isinstance(exp_dir, str):\n",
    "        exp_dir = Path(\"experiments\") / exp_dir\n",
    "    elif isinstance(exp_dir, Path):\n",
    "        exp_dir = Path(\"experiments\") / exp_dir.name\n",
    "    \n",
    "    exp_dir = exp_dir.absolute()\n",
    "    logger.info(f\"Looking for data in: {exp_dir}\")\n",
    "    \n",
    "    # Create seaborn plots directory\n",
    "    seaborn_dir = exp_dir / \"seaborn_plots\"\n",
    "    seaborn_dir.mkdir(exist_ok=True)\n",
    "    logger.info(f\"Created seaborn plots directory: {seaborn_dir}\")\n",
    "    \n",
    "    # Initialize lists to store all data\n",
    "    all_metrics = []\n",
    "    all_task_data = []\n",
    "    \n",
    "    # Iterate through model configuration directories\n",
    "    data_dir = exp_dir / \"intermediate_data\"\n",
    "    model_dirs = list(data_dir.glob(\"*x*\"))\n",
    "    logger.info(f\"Found {len(model_dirs)} model configuration directories\")\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        config_str = model_dir.name\n",
    "        logger.info(f\"Processing data for model configuration: {config_str}\")\n",
    "        \n",
    "        # Get metrics files for this configuration\n",
    "        metrics_files = sorted(model_dir.glob(\"metrics_*.csv\"))\n",
    "        logger.info(f\"Found {len(metrics_files)} metrics files for {config_str}\")\n",
    "        \n",
    "        for file in metrics_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                df['model_config'] = config_str\n",
    "                all_metrics.append(df)\n",
    "                logger.debug(f\"Processed metrics file: {file.name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing metrics file {file}: {e}\")\n",
    "        \n",
    "        # Get task accuracy files\n",
    "        task_files = sorted(model_dir.glob(\"task_accuracies_*.csv\"))\n",
    "        logger.info(f\"Found {len(task_files)} task accuracy files for {config_str}\")\n",
    "        \n",
    "        for file in task_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                task_cols = [col for col in df.columns if 'task' in col]\n",
    "                for i in range(0, len(task_cols), 2):\n",
    "                    flops_col = task_cols[i]\n",
    "                    acc_col = task_cols[i+1]\n",
    "                    task_num = flops_col.split('_')[1]\n",
    "                    \n",
    "                    task_df = pd.DataFrame({\n",
    "                        'flops': df[flops_col],\n",
    "                        'accuracy': df[acc_col],\n",
    "                        'task': f'Task {task_num}',\n",
    "                        'model_config': config_str\n",
    "                    })\n",
    "                    all_task_data.append(task_df)\n",
    "                logger.debug(f\"Processed task file: {file.name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing task file {file}: {e}\")\n",
    "    \n",
    "    # Check if we have data to plot\n",
    "    if not all_metrics:\n",
    "        logger.error(\"No metrics data found\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Combining {len(all_metrics)} metric dataframes\")\n",
    "    metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    logger.info(f\"Combined metrics shape: {metrics_df.shape}\")\n",
    "    \n",
    "    # Create epoch bins\n",
    "    try:\n",
    "        metrics_df['epoch_bin'] = pd.qcut(metrics_df['flops'], \n",
    "                                        q=len(metrics_df)//epoch_window, \n",
    "                                        labels=False,\n",
    "                                        duplicates='drop')\n",
    "        logger.info(\"Created epoch bins for metrics\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating epoch bins for metrics: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Combine task data if available\n",
    "    if all_task_data:\n",
    "        logger.info(f\"Combining {len(all_task_data)} task dataframes\")\n",
    "        task_df = pd.concat(all_task_data, ignore_index=True)\n",
    "        logger.info(f\"Combined task data shape: {task_df.shape}\")\n",
    "        \n",
    "        try:\n",
    "            task_df['epoch_bin'] = pd.qcut(task_df['flops'], \n",
    "                                         q=len(task_df)//epoch_window, \n",
    "                                         labels=False,\n",
    "                                         duplicates='drop')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating epoch bins for tasks: {e}\")\n",
    "    \n",
    "    # Create plots\n",
    "    logger.info(\"Creating plots\")\n",
    "    try:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Create and save each plot separately\n",
    "        # 1. Loss plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=metrics_df, \n",
    "                    x='flops', \n",
    "                    y='loss',\n",
    "                    hue='model_config',\n",
    "                    errorbar=('ci', confidence_interval))\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.title('Training Loss vs FLOPs')\n",
    "        loss_path = seaborn_dir / f\"loss_plot_window{epoch_window}.png\"\n",
    "        plt.savefig(loss_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved loss plot to {loss_path}\")\n",
    "        \n",
    "        # 2. Accuracy plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=metrics_df, \n",
    "                    x='flops', \n",
    "                    y='accuracy',\n",
    "                    hue='model_config',\n",
    "                    errorbar=('ci', confidence_interval))\n",
    "        plt.xscale('log')\n",
    "        plt.title('Overall Accuracy vs FLOPs')\n",
    "        acc_path = seaborn_dir / f\"accuracy_plot_window{epoch_window}.png\"\n",
    "        plt.savefig(acc_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved accuracy plot to {acc_path}\")\n",
    "        \n",
    "        # 3. Task-specific plot\n",
    "        if all_task_data:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.lineplot(data=task_df, \n",
    "                        x='flops', \n",
    "                        y='accuracy',\n",
    "                        hue='task',\n",
    "                        style='model_config',\n",
    "                        errorbar=('ci', confidence_interval))\n",
    "            plt.xscale('log')\n",
    "            plt.title('Task-Specific Accuracy vs FLOPs')\n",
    "            task_path = seaborn_dir / f\"task_accuracy_plot_window{epoch_window}.png\"\n",
    "            plt.savefig(task_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved task-specific plot to {task_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating plots: {e}\")\n",
    "        raise\n",
    "    \n",
    "    logger.info(\"Finished creating all plots\")\n",
    "\n",
    "    \n",
    "#create_seaborn_plots(Path('parity_scaling_flops_1e+10__20250108_000607_v1'))\n",
    "exp_dir = Path('/experiments/parity_scaling_flops_1e+10__20250108_000607_v1')\n",
    "# When calling the function\n",
    "#print(f\"Original exp_dir: {exp_dir}\")\n",
    "#print(f\"Absolute exp_dir: {exp_dir.resolve()}\")\n",
    "create_seaborn_plots(exp_dir.resolve(), epoch_window = 100) # larger epoch windows result in slowdown\n",
    "#create_seaborn_plots(Path('workspace/project/experiments/parity_scaling_flops_1e+10__20250108_000607_v1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
