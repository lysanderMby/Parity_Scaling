{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of test of parity scaling laws in Jupyter notebook.\n",
    "\n",
    "The code contained here is very similar to that in the rest of the repo, but is not guarenteed to be identical.\n",
    "\n",
    "This notebook is included for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "\n",
    "# pip install if necessary\n",
    "'''\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install pathlib\n",
    "!pip install logging\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Parameters\n",
    "PARAMS = {\n",
    "    'n_tasks': 1, # number of unique tasks being trained over\n",
    "    'len_taskcode': 4, # number of bits in the task code\n",
    "    'num_checks': 5, # number of bits in the message that are used to determine the output\n",
    "    'len_message': 16, # number of bits in the message\n",
    "    'num_samples': 1000, # number of samples to generate for each task\n",
    "    'input_size': 20,  # len_taskcode + len_message. Used for model initialisation\n",
    "    'output_size': 1, # output size of the model. 1 for binary classification. Do not change\n",
    "    'learning_rate': 0.005, # constant learning rate. Could introduce a scheduler?\n",
    "    'batch_size': 32, # batch size used in training. Will be the same throughout a run\n",
    "    'flop_budget': 1e10, # total number of estimated flops expended per training run\n",
    "    'task_sample_freq': 1e5,  # the rate at which performance is evaluated. Can give a big performance hit\n",
    "    'plot_freq': 2e7,  # flop_budget/5\n",
    "    'samples_per_task': 100 # number of samples to generate for each task in evaluation\n",
    "}\n",
    "\n",
    "# Define a range of model configurations\n",
    "model_configs = [\n",
    "    {\"num_layers\": 2, \"hidden_size\": 8},\n",
    "    {\"num_layers\": 4, \"hidden_size\": 16},\n",
    "    {\"num_layers\": 6, \"hidden_size\": 32},\n",
    "    {\"num_layers\": 8, \"hidden_size\": 64},\n",
    "    {\"num_layers\": 10, \"hidden_size\": 128},\n",
    "    {\"num_layers\": 12, \"hidden_size\": 256},\n",
    "    {\"num_layers\": 14, \"hidden_size\": 512} \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "    \n",
    "def generate_random_binary_string(length):\n",
    "    # Random binary string of length 'length'\n",
    "    return ''.join(random.choice(['0', '1']) for _ in range(length))\n",
    "\n",
    "def generate_dict(n_tasks, len_taskcode, num_checks, len_message):\n",
    "    # Generate the task codes and their associated check bits\n",
    "    unique_strings = set()\n",
    "    tasks_dict = {}\n",
    "    while len(unique_strings) < n_tasks:\n",
    "        binary_string = generate_random_binary_string(len_taskcode)\n",
    "        if binary_string not in unique_strings:\n",
    "            unique_strings.add(binary_string)\n",
    "            integer_list = [random.randint(0, len_message-1) for _ in range(num_checks)]\n",
    "            tasks_dict[binary_string] = integer_list\n",
    "    return tasks_dict\n",
    "\n",
    "def generate_dataset(tasks_dict, num_samples, len_taskcode, len_message):\n",
    "    # Generate a dataset of num_samples samples using tasks specified in tasks_dict\n",
    "    data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "    value = np.zeros(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        rand_task = np.random.choice(list(tasks_dict))\n",
    "        rand_checkbits = tasks_dict[rand_task]\n",
    "        message = generate_random_binary_string(len_message)\n",
    "        parity_bit = sum(int(message[j]) for j in rand_checkbits) % 2\n",
    "        data[i] = np.concatenate((np.array(list(rand_task)), np.array(list(message))))\n",
    "        value[i] = parity_bit\n",
    "    return [data, value]\n",
    "\n",
    "def generate_dataset_for_task(task_code, tasks_dict, num_samples, len_taskcode, len_message):\n",
    "    # Generate a dataset of num_samples samples for a specific task\n",
    "    # Used primarily for evaluation. Very limited performance improvement from generate_dataset\n",
    "    data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "    value = np.zeros(num_samples)\n",
    "    rand_checkbits = tasks_dict[task_code]\n",
    "    for i in range(num_samples):\n",
    "        message = generate_random_binary_string(len_message)\n",
    "        parity_bit = sum(int(message[j]) for j in rand_checkbits) % 2\n",
    "        data[i] = np.concatenate((np.array(list(task_code)), np.array(list(message))))\n",
    "        value[i] = parity_bit\n",
    "    return [data, value]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, device):\n",
    "        # Convert to numpy first for efficiency\n",
    "        data_np = dataframe.iloc[:, :-1].values\n",
    "        target_np = dataframe.iloc[:, -1].values\n",
    "        \n",
    "        # Single transfer to device. IO-aware for greater efficiency\n",
    "        self.data = torch.from_numpy(data_np).float().to(device)\n",
    "        self.target = torch.from_numpy(target_np).float().to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            if i == 0:\n",
    "                x = F.relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                if i % 2 == 0:\n",
    "                    x = F.relu(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot intermediate progress\n",
    "\n",
    "def plot_progress(loss_data, accuracy_data, task_accuracy_data, cumulative_flops, exp_dir):\n",
    "    \"\"\"\n",
    "    Plot and save training progress.\n",
    "    \n",
    "    Args:\n",
    "        loss_data: List of tuples (flops, loss)\n",
    "        accuracy_data: List of tuples (flops, accuracy)\n",
    "        task_accuracy_data: Dict of lists of tuples (flops, accuracy) for each task\n",
    "        cumulative_flops: Current total FLOPs\n",
    "        exp_dir: Path to experiment directory\n",
    "    \"\"\"\n",
    "    # Create plots directory if it doesn't exist\n",
    "    plots_dir = exp_dir / \"intermediate_plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    flops_loss, losses = zip(*loss_data)\n",
    "    ax1.plot(flops_loss, losses)\n",
    "    ax1.set_xlabel('FLOPs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss vs FLOPs')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot overall accuracy\n",
    "    flops_acc, accuracies = zip(*accuracy_data)\n",
    "    ax2.plot(flops_acc, accuracies)\n",
    "    ax2.set_xlabel('FLOPs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Overall Accuracy vs FLOPs')\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    # Plot task-specific accuracies\n",
    "    for task_idx, task_data in task_accuracy_data.items():\n",
    "        if task_data:  # Check if there's data for this task\n",
    "            flops_task, task_accuracies = zip(*task_data)\n",
    "            ax3.plot(flops_task, task_accuracies, label=f'Task {task_idx}')\n",
    "    ax3.set_xlabel('FLOPs')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_title('Task-Specific Accuracy vs FLOPs')\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = plots_dir / f\"progress_{cumulative_flops:.2e}_flops.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Log the plot creation\n",
    "    logging.info(f\"Saved progress plot at {cumulative_flops:.2e} FLOPs to {plot_path}\")\n",
    "    \n",
    "    # Also save the data as CSV for later analysis\n",
    "    data_dir = exp_dir / \"intermediate_data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save loss and accuracy data\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'flops': flops_loss,\n",
    "        'loss': losses,\n",
    "        'accuracy': accuracies\n",
    "    })\n",
    "    df_metrics.to_csv(data_dir / f\"metrics_{cumulative_flops:.2e}_flops.csv\", index=False)\n",
    "    \n",
    "    # Save task-specific accuracy data\n",
    "    task_data_dict = {}\n",
    "    for task_idx, task_data in task_accuracy_data.items():\n",
    "        if task_data:\n",
    "            flops_task, task_accuracies = zip(*task_data)\n",
    "            task_data_dict[f'task_{task_idx}_flops'] = flops_task\n",
    "            task_data_dict[f'task_{task_idx}_accuracy'] = task_accuracies\n",
    "    \n",
    "    df_tasks = pd.DataFrame(task_data_dict)\n",
    "    df_tasks.to_csv(data_dir / f\"task_accuracies_{cumulative_flops:.2e}_flops.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main plot\n",
    "\n",
    "def main_plot(all_loss_data, all_accuracy_data, all_task_accuracy_data, all_flops):\n",
    "\n",
    "    # Create final plots\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Remove the last element of each loss list. The final step of evaluation is always cut short and not the entire batch is used?\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for i, config in enumerate(model_configs):\n",
    "        flops, losses = zip(*all_loss_data[i])\n",
    "        plt.loglog(flops, losses, label=f'{config[\"num_layers\"]}x{config[\"hidden_size\"]}')\n",
    "    plt.xlabel('Cumulative FLOPs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs FLOPs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for i, config in enumerate(model_configs):\n",
    "        flops, accuracies = zip(*all_accuracy_data[i])\n",
    "        plt.semilogx(flops, accuracies, label=f'{config[\"num_layers\"]}x{config[\"hidden_size\"]}')\n",
    "    plt.xlabel('Cumulative FLOPs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs FLOPs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for i, config in enumerate(model_configs):\n",
    "        for task in range(n_tasks):\n",
    "            flops, accuracies = zip(*all_task_accuracy_data[i][task])\n",
    "            plt.semilogx(flops, accuracies, label=f'Task {task+1} - {config[\"num_layers\"]}x{config[\"hidden_size\"]}')\n",
    "    plt.xlabel('Cumulative FLOPs')\n",
    "    plt.ylabel('Task-specific Accuracy')\n",
    "    plt.title('Task-specific Accuracies vs FLOPs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count FLOPs\n",
    "from typing import Tuple, Dict # if used, this should be moved to the import section\n",
    "\n",
    "# No longer in use\n",
    "#def count_flops_fvcore(model, input_size):\n",
    "#    input_tensor = torch.randn(2, input_size)\n",
    "#    flops = FlopCountAnalysis(model, input_tensor)\n",
    "#    return flops.total() // 2\n",
    "\n",
    "class FlopCounter:\n",
    "    def __init__(self, model: nn.Module, input_size: int, batch_size: int):\n",
    "        self.model = model\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def count_linear_flops(self, in_features: int, out_features: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count FLOPs for linear layer operations.\n",
    "        Forward: Each output element requires in_features multiplications and in_features-1 additions\n",
    "        Backward: Requires computing gradients for weights, biases, and input\n",
    "        \"\"\"\n",
    "        forward_flops = self.batch_size * out_features * (2 * in_features - 1)  # mult-add pairs\n",
    "        \n",
    "        # Backward pass FLOPs:\n",
    "        # 1. dL/dW computation: batch_size * in_features * out_features * 2\n",
    "        # 2. dL/db computation: batch_size * out_features\n",
    "        # 3. dL/dx computation: batch_size * in_features * out_features * 2\n",
    "        backward_flops = (\n",
    "            self.batch_size * in_features * out_features * 2 +  # dL/dW\n",
    "            self.batch_size * out_features +                    # dL/db\n",
    "            self.batch_size * in_features * out_features * 2    # dL/dx\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"forward\": forward_flops,\n",
    "            \"backward\": backward_flops\n",
    "        }\n",
    "    \n",
    "    def count_batch_norm_flops(self, num_features: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count FLOPs for batch normalization operations.\n",
    "        Forward: Computing mean, variance, normalized values, and scaling\n",
    "        Backward: Computing gradients for gamma, beta, and input\n",
    "        \"\"\"\n",
    "        # Forward pass operations per feature:\n",
    "        # 1. Mean calculation: batch_size additions\n",
    "        # 2. Variance calculation: batch_size multiplications and additions\n",
    "        # 3. Normalization: 4 operations per element (subtract mean, divide by std)\n",
    "        # 4. Scale and shift: 2 operations per element\n",
    "        forward_flops = self.batch_size * num_features * (7)\n",
    "        \n",
    "        # Backward pass operations:\n",
    "        # 1. Gradients for gamma and beta: batch_size additions per feature\n",
    "        # 2. Gradients for input: ~8 operations per element\n",
    "        backward_flops = self.batch_size * num_features * 10\n",
    "        \n",
    "        return {\n",
    "            \"forward\": forward_flops,\n",
    "            \"backward\": backward_flops\n",
    "        }\n",
    "    \n",
    "    def count_relu_flops(self, num_elements: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count FLOPs for ReLU activation.\n",
    "        Forward: One comparison per element\n",
    "        Backward: One multiplication per element (gradient is 0 or 1)\n",
    "        \"\"\"\n",
    "        forward_flops = num_elements  # One comparison per element\n",
    "        backward_flops = num_elements  # One multiplication per element\n",
    "        \n",
    "        return {\n",
    "            \"forward\": forward_flops,\n",
    "            \"backward\": backward_flops\n",
    "        }\n",
    "    \n",
    "    def calculate_total_flops(self) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculate total FLOPs for both forward and backward passes through the entire model.\n",
    "        Returns tuple of (forward_flops, backward_flops)\n",
    "        \"\"\"\n",
    "        total_forward_flops = 0\n",
    "        total_backward_flops = 0\n",
    "        \n",
    "        current_size = self.input_size\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                flops = self.count_linear_flops(layer.in_features, layer.out_features)\n",
    "                total_forward_flops += flops[\"forward\"]\n",
    "                total_backward_flops += flops[\"backward\"]\n",
    "                current_size = layer.out_features\n",
    "                \n",
    "            elif isinstance(layer, nn.BatchNorm1d):\n",
    "                flops = self.count_batch_norm_flops(current_size)\n",
    "                total_forward_flops += flops[\"forward\"]\n",
    "                total_backward_flops += flops[\"backward\"]\n",
    "                \n",
    "            # Count ReLU FLOPs after linear layers (except the last one)\n",
    "            if isinstance(layer, nn.Linear) and layer != self.model.layers[-1]:\n",
    "                flops = self.count_relu_flops(self.batch_size * current_size)\n",
    "                total_forward_flops += flops[\"forward\"]\n",
    "                total_backward_flops += flops[\"backward\"]\n",
    "        \n",
    "        return total_forward_flops, total_backward_flops\n",
    "\n",
    "def get_flops_per_pass(model: nn.Module, input_size: int, batch_size: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Wrapper function to get FLOPs per forward and backward pass.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        input_size: Size of input features\n",
    "        batch_size: Batch size used in training\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (forward_flops, backward_flops)\n",
    "    \"\"\"\n",
    "    counter = FlopCounter(model, input_size, batch_size)\n",
    "    return counter.calculate_total_flops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# This function has been significantly modified to incorporate the new count FLOPs function\n",
    "# There may be bugs down the line related to how data is collected and displayed. This should be more thoroughly debugged\n",
    "\n",
    "def train_and_evaluate(model, criterion, optimizer, flop_budget, tasks_dict):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize FlopCounter\n",
    "    flop_counter = FlopCounter(model, input_size=len_taskcode + len_message, batch_size=batch_size)\n",
    "    forward_flops, backward_flops = flop_counter.calculate_total_flops()\n",
    "    \n",
    "    loss_data = []\n",
    "    accuracy_data = []\n",
    "    task_accuracy_data = {i: [] for i in range(n_tasks)}\n",
    "    cumulative_flops = 0\n",
    "    epoch = 0\n",
    "    last_task_sample = 0\n",
    "    last_plot = 0\n",
    "\n",
    "    print_rate = flop_budget / 1e1\n",
    "    disp_flops = 0\n",
    "\n",
    "    while cumulative_flops < flop_budget:\n",
    "        if cumulative_flops - print_rate > disp_flops:\n",
    "            print(f'cumulative_flops: {cumulative_flops} - flop_budget: {flop_budget} - Percentage completion: {(cumulative_flops/flop_budget)*100:.2f}%')\n",
    "            disp_flops = cumulative_flops\n",
    "            \n",
    "        epoch += 1\n",
    "        [data, value] = generate_dataset(tasks_dict, num_samples)\n",
    "        df = pd.DataFrame(np.concatenate((data, value.reshape(-1, 1)), axis=1), \n",
    "                         columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "        \n",
    "        dataset = CustomDataset(df, device)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            batch_loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            predictions = (outputs >= 0.5).squeeze().long()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += batch_loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate FLOPs\n",
    "            batch_flops = forward_flops + backward_flops\n",
    "            batch_flops += forward_flops + backward_flops\n",
    "            cumulative_flops += batch_flops\n",
    "\n",
    "            if cumulative_flops >= flop_budget and i > (len(data_loader) - 1):\n",
    "                break\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataset)\n",
    "        avg_accuracy = correct / total\n",
    "\n",
    "        loss_data.append((cumulative_flops, avg_loss))\n",
    "        accuracy_data.append((cumulative_flops, avg_accuracy))\n",
    "        \n",
    "        # Task-specific evaluation\n",
    "        if cumulative_flops - last_task_sample >= task_sample_freq or cumulative_flops >= flop_budget:\n",
    "            last_task_sample = cumulative_flops\n",
    "            tasks_list = list(tasks_dict.keys())\n",
    "            \n",
    "            for task_idx, task_code in enumerate(tasks_list):\n",
    "                [data_per_task, value_per_task] = generate_dataset_for_task(task_code, tasks_dict, samples_per_task)\n",
    "                df_per_task = pd.DataFrame(np.concatenate((data_per_task, value_per_task.reshape(-1, 1)), axis=1), \n",
    "                                         columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "                dataset_per_task = CustomDataset(df_per_task, device)\n",
    "                loader_per_task = DataLoader(dataset_per_task, batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "                model.eval()\n",
    "                task_correct = 0\n",
    "                task_total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in loader_per_task:\n",
    "                        outputs = model(inputs)\n",
    "                        predictions = (outputs >= 0.5).squeeze().long()\n",
    "                        task_correct += (predictions == labels).sum().item()\n",
    "                        task_total += labels.size(0)\n",
    "                        cumulative_flops += forward_flops\n",
    "                        \n",
    "                task_accuracy = task_correct / task_total\n",
    "                task_accuracy_data[task_idx].append((cumulative_flops, task_accuracy))\n",
    "        \n",
    "        if cumulative_flops - last_plot >= plot_freq:\n",
    "            last_plot = cumulative_flops\n",
    "            plot_progress(loss_data, accuracy_data, task_accuracy_data, cumulative_flops, exp_dir)\n",
    "\n",
    "    return loss_data, accuracy_data, task_accuracy_data, cumulative_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create experiment directory\n",
    "    exp_name = f\"parity_scaling_flops_{PARAMS['flop_budget']:.0e}\"\n",
    "    exp_dir = create_versioned_directory(Path(\"experiments\"), exp_name)\n",
    "    print(f\"Experiment directory: {exp_dir}\")\n",
    "\n",
    "    print(f\"FLOP budget: {PARAMS['flop_budget']}\")\n",
    "    \n",
    "    tasks_dict = generate_dict(\n",
    "        PARAMS['n_tasks'], \n",
    "        PARAMS['len_taskcode'], \n",
    "        PARAMS['num_checks'], \n",
    "        PARAMS['len_message']\n",
    "    )\n",
    "    print(f\"Generated tasks dictionary with {len(tasks_dict)} tasks\")\n",
    "    print(\"tasks_dict = \", tasks_dict.items())\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Add progress bar for model configurations\n",
    "    for config in tqdm(MODEL_CONFIGS, desc=\"Training models\", position=0, leave=True):\n",
    "        print(f\"\\nTraining model with {config['num_layers']} layers and hidden size {config['hidden_size']}\")\n",
    "        model = NeuralNetwork(\n",
    "            PARAMS['input_size'], \n",
    "            PARAMS['output_size'], \n",
    "            config[\"num_layers\"], \n",
    "            config[\"hidden_size\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        results = train_and_evaluate(\n",
    "            model=model,\n",
    "            params=PARAMS,\n",
    "            tasks_dict=tasks_dict,\n",
    "            exp_dir=exp_dir,\n",
    "            model_config=config\n",
    "        )\n",
    "        all_results.append(results)\n",
    "    \n",
    "    # Create final plots\n",
    "    main_plot(all_results, exp_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional post-processing function\n",
    "\n",
    "def create_seaborn_plots(exp_dir: Path, epoch_window: int = 10, confidence_interval: float = 0.95):\n",
    "    \"\"\"Create seaborn plots from saved data.\"\"\"\n",
    "    # Convert string to Path if necessary\n",
    "    if isinstance(exp_dir, str):\n",
    "        exp_dir = Path(\"experiments\") / exp_dir\n",
    "    elif isinstance(exp_dir, Path):\n",
    "        exp_dir = Path(\"experiments\") / exp_dir.name\n",
    "    \n",
    "    exp_dir = exp_dir.absolute()\n",
    "    \n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Looking for directory: {exp_dir}\")\n",
    "    \n",
    "    # Verify experiment directory exists\n",
    "    if not exp_dir.exists():\n",
    "        print(\"\\nContents of experiments directory:\")\n",
    "        try:\n",
    "            experiments_dir = Path(\"experiments\")\n",
    "            for item in experiments_dir.iterdir():\n",
    "                print(f\"  {item.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing experiments directory: {e}\")\n",
    "        raise ValueError(f\"Experiment directory does not exist: {exp_dir}\")\n",
    "    \n",
    "    # Create seaborn plots directory\n",
    "    seaborn_dir = exp_dir / \"seaborn_plots\"\n",
    "    seaborn_dir.mkdir(exist_ok=True)\n",
    "    logging.info(f\"Created seaborn directory at: {seaborn_dir}\")\n",
    "    \n",
    "    # Verify data directory exists\n",
    "    data_dir = exp_dir / \"intermediate_data\"\n",
    "    if not data_dir.exists():\n",
    "        raise ValueError(f\"Data directory does not exist: {data_dir}\")\n",
    "    \n",
    "    # Get all metrics files\n",
    "    metrics_files = sorted(data_dir.glob(\"metrics_*.csv\"))\n",
    "    task_files = sorted(data_dir.glob(\"task_accuracies_*.csv\"))\n",
    "    \n",
    "    logging.info(f\"Found {len(metrics_files)} metrics files and {len(task_files)} task files\")\n",
    "    \n",
    "    if not metrics_files:\n",
    "        raise ValueError(f\"No metrics files found in {data_dir}\")\n",
    "    #######\n",
    "    # Combine all metrics data\n",
    "    all_metrics = []\n",
    "    for file in metrics_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_metrics.append(df)\n",
    "    metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    \n",
    "    # Create epoch bins for averaging, handling duplicates\n",
    "    try:\n",
    "        metrics_df['epoch_bin'] = pd.qcut(metrics_df['flops'], \n",
    "                                        q=len(metrics_df)//epoch_window, \n",
    "                                        labels=False,\n",
    "                                        duplicates='drop')\n",
    "    except ValueError:\n",
    "        # If qcut fails, use regular cut with logarithmic bins\n",
    "        n_bins = len(metrics_df)//epoch_window\n",
    "        metrics_df['epoch_bin'] = pd.cut(np.log10(metrics_df['flops']),\n",
    "                                       bins=n_bins,\n",
    "                                       labels=False)\n",
    "    \n",
    "    # Task - specific accuracy data\n",
    "    all_task_data = []\n",
    "    for file in task_files:\n",
    "        df = pd.read_csv(file)\n",
    "        task_cols = [col for col in df.columns if 'task' in col]\n",
    "        for i in range(0, len(task_cols), 2):\n",
    "            flops_col = task_cols[i]\n",
    "            acc_col = task_cols[i+1]\n",
    "            task_num = flops_col.split('_')[1]\n",
    "            \n",
    "            task_df = pd.DataFrame({\n",
    "                'flops': df[flops_col],\n",
    "                'accuracy': df[acc_col],\n",
    "                'task': f'Task {task_num}'\n",
    "            })\n",
    "            all_task_data.append(task_df)\n",
    "    \n",
    "    task_df = pd.concat(all_task_data, ignore_index=True)\n",
    "    try:\n",
    "        task_df['epoch_bin'] = pd.qcut(task_df['flops'], \n",
    "                                     q=len(task_df)//epoch_window, \n",
    "                                     labels=False,\n",
    "                                     duplicates='drop')\n",
    "    except ValueError:\n",
    "        n_bins = len(task_df)//epoch_window\n",
    "        task_df['epoch_bin'] = pd.cut(np.log10(task_df['flops']),\n",
    "                                    bins=n_bins,\n",
    "                                    labels=False)\n",
    "        ########\n",
    "\n",
    "    # Set up the plotting style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # 1. Loss plot\n",
    "    sns.lineplot(data=metrics_df, \n",
    "                x='flops', \n",
    "                y='loss',\n",
    "                errorbar=('ci', confidence_interval),\n",
    "                ax=ax1)\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_title('Training Loss vs FLOPs')\n",
    "    ax1.set_xlabel('FLOPs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    # 2. Accuracy plot\n",
    "    sns.lineplot(data=metrics_df, \n",
    "                x='flops', \n",
    "                y='accuracy',\n",
    "                errorbar=('ci', confidence_interval),\n",
    "                ax=ax2)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_title('Overall Accuracy vs FLOPs')\n",
    "    ax2.set_xlabel('FLOPs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    sns.lineplot(data=task_df, \n",
    "                x='flops', \n",
    "                y='accuracy',\n",
    "                hue='task',\n",
    "                errorbar=('ci', confidence_interval),\n",
    "                ax=ax3)\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.set_title('Task-Specific Accuracy vs FLOPs')\n",
    "    ax3.set_xlabel('FLOPs')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = seaborn_dir / f\"seaborn_summary_window{epoch_window}_ci{confidence_interval}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Also create separate plots for each metric with error bands\n",
    "    metrics = ['loss', 'accuracy']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=metrics_df, \n",
    "                    x='flops', \n",
    "                    y=metric,\n",
    "                    errorbar=('ci', confidence_interval))\n",
    "        plt.xscale('log')\n",
    "        if metric == 'loss':\n",
    "            plt.yscale('log')\n",
    "        plt.title(f'{metric.capitalize()} vs FLOPs')\n",
    "        plt.xlabel('FLOPs')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.tight_layout()\n",
    "        plot_path = seaborn_dir / f\"seaborn_{metric}_window{epoch_window}_ci{confidence_interval}.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    # Create task-specific plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=task_df, \n",
    "                x='flops', \n",
    "                y='accuracy',\n",
    "                hue='task',\n",
    "                errorbar=('ci', confidence_interval))\n",
    "    plt.xscale('log')\n",
    "    plt.title('Task-Specific Accuracy vs FLOPs')\n",
    "    plt.xlabel('FLOPs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plot_path = seaborn_dir / f\"seaborn_task_accuracy_window{epoch_window}_ci{confidence_interval}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"Created seaborn plots in {seaborn_dir}\")\n",
    "\n",
    "#create_seaborn_plots(Path('parity_scaling_flops_1e+10__20250108_000607_v1'))\n",
    "exp_dir = Path('/experiments/parity_scaling_flops_1e+10__20250108_000607_v1')\n",
    "# When calling the function\n",
    "#print(f\"Original exp_dir: {exp_dir}\")\n",
    "#print(f\"Absolute exp_dir: {exp_dir.resolve()}\")\n",
    "create_seaborn_plots(exp_dir.resolve(), epoch_window = 100) # larger epoch windows result in slowdown\n",
    "#create_seaborn_plots(Path('workspace/project/experiments/parity_scaling_flops_1e+10__20250108_000607_v1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
